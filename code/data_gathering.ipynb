{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ec82b7-208f-4781-84c3-09b6e6fc6352",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd24a20-7ff6-4a6a-9f53-df2a82c9cbaa",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85348bde-903a-41a3-94f7-f014d8fdd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d7e1-3317-40bd-b8a7-64413b7fdcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ded0070-481c-4ae2-acba-2cf09f39ca96",
   "metadata": {},
   "source": [
    "### Function to scrape data from subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450102c-5f4f-4fd2-aa3f-4635605dc61c",
   "metadata": {},
   "source": [
    "##### My intent is to examine data from 2 subreddits over the same period of time.  However, one subreddit is much more heavily trafficked, so I may need to switch my approach to gathering a specified number of posts from each subreddit.  I'm building a data-gathering function for each case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b936c-05f6-450e-a2b5-57dcce15c8ea",
   "metadata": {},
   "source": [
    "#### Function to scrape data based on Start and End dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103039b-9f40-4050-9b7a-51f5ec1d67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1646092801 == 12:00:00AM GMT 3/1/22\n",
    "#1650844800 == 12:00:00AM GMT 4/25/22, \n",
    "#function to scrape submissions\n",
    "def scrape_time(subreddit, start_time=1646092801, end_time=1650844800):\n",
    "    start_time = start_time\n",
    "    end_time = end_time\n",
    "    full_post_list = []\n",
    "    \n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': end_time  }\n",
    "    \n",
    "    while end_time > start_time:\n",
    "        res = requests.get('https://api.pushshift.io/reddit/search/submission?subreddit', params)\n",
    "        data = res.json()\n",
    "        data_df = pd.json_normalize(data['data'])\n",
    "        posts = data_df[['author', 'created_utc', 'domain', 'url', 'title', 'subreddit']]\n",
    "        \n",
    "        posts_df = pd.DataFrame(posts)\n",
    "        full_post_list.append(posts_df)\n",
    "        \n",
    "        end_time = posts_df['created_utc'].min()\n",
    "        params['before'] = end_time\n",
    "        time.sleep(2)\n",
    "        df = pd.concat(full_post_list)\n",
    "        \n",
    "    return df.to_csv(f'../data/{subreddit}_posts_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22ab9d-551f-4cd2-9868-eabc43f0310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_time('upliftingnews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49348d3c-b091-48be-a008-d9d21889c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_time('worldnews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d01d9c-4905-4971-9979-61292af37150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6038e5b8-64e0-4b3e-8f1c-037f77d90ad6",
   "metadata": {},
   "source": [
    "#### Function to scrape data for a specified number of loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14834daf-4fcd-4645-8297-f4bd7d6a8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1650844800 == 12:00:00AM GMT 4/25/22, \n",
    "#function to scrape submissions\n",
    "def scrape_loops(subreddit, loops, end_time=1650844800):\n",
    "    full_post_list = []\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': end_time }\n",
    "    \n",
    "    for i in range(loops):\n",
    "        res = requests.get('https://api.pushshift.io/reddit/search/submission?subreddit', params)\n",
    "        data = res.json()\n",
    "        data_df = pd.json_normalize(data['data'])\n",
    "        posts = data_df[['author', 'created_utc', 'domain', 'url', 'title', 'subreddit']]\n",
    "        \n",
    "        posts_df = pd.DataFrame(posts)\n",
    "        full_post_list.append(posts_df)\n",
    "        \n",
    "        oldest = posts_df['created_utc'].min()\n",
    "        params['before'] = oldest\n",
    "        time.sleep(2)\n",
    "        df = pd.concat(full_post_list)\n",
    "        \n",
    "    return df.to_csv(f'../data/{subreddit}_posts_loop.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403aa512-6b11-44c7-8ddc-505b49660121",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_loops('upliftingnews', 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7cf905-aa43-4058-9716-a12aa4894c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_loops('worldnews', 41)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
